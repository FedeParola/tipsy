#+LaTeX_HEADER:\usepackage[margin=2cm]{geometry}
#+LaTeX_HEADER:\usepackage{enumitem}
#+LaTeX_HEADER:\usepackage{tikz}
#+LATEX:\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
#+OPTIONS: toc:nil ^:nil num:nil

#+TITLE: Vision: The TIPSY Workflow

Unfortunately, TIPSY is still unfinished.  The current vision for how a
full-fledged TIPSY would work is as follows.  Take this as a pretty much
unfinished design document for now on how we envision TIPSY would work when
it becomes ready.

1. Create the benchmark root directory.

   #+BEGIN_SRC sh
   mkdir my_bng_benchmark
   cd my_bng_benchmark
   #+END_SRC

2. Place a main TIPSY configuration file =my_benchmark.json= into the root
   directory and edit it according to your needs. The below configuration
   will set up the =MGW= pipeline and benchmark the OVS and the BESS
   backends, while scaling the number of users from 10 to 1000.

   #+BEGIN_SRC javascript
     {
         "benchmark": [
             {
                 "id": "test1"
                 "scale": "outer",
                 "pipeline": {
                     "name": "mgw",
                     "user": [10, 100, 1000, 1000]
                 }
             }
         ],
         "traffic": {
             "pkt-size": 64
         },
         "sut": {
             "type": ["ovs", "bess"]
         },
         "tester": {
             "generator": "moongen",
	     "test-time": 30
         },
         "visualize":
         [
             {
                 "x_axis": "pipeline.user",
                 "y_axis": "mg.throughput.RX.PacketRate",
                 "group-by": "sut.type"
             }
         ]
     }
   #+END_SRC

3. Validate your benchmark configuration against TIPSY schemas.

   #+BEGIN_SRC sh
   tipsy validate my_benchmark.json
   #+END_SRC

4. Generate the configuration for the individual test cases that make up
   the benchmark, that is, a separate test for all settings of the =user=
   and =backend= parameters, with each test case configuration placed into
   a separate directory.

   #+BEGIN_SRC sh
   tipsy config my_benchmark.json
   #+END_SRC

   This call will set the benchmark configuration from =my_benchmark.json=,
   setting each parameter that was not explicitly specified there to a sane
   default value.

   Optionally, you may let TIPSY to create the benchmark by merging
   multiple JSON configs.

   #+BEGIN_SRC sh
   tipsy config /var/lib/tipsy/environment.json /var/lib/dpdk/dpdk.json my_benchmark.json
   #+END_SRC

   The order of JSON files on the command line specifies the override
   priority.  This allows to use, say, a common =environment.json= to set
   the filesystem paths for the backends or a =dpdk.json= configuration to
   contain all local DPDK settings.

   Finally, you can also omit all JSON files in which case TIPSY will take
   all JSON files in the current directory, merge them into a single JSON
   and use that as a configuration for the benchmark.

   #+BEGIN_SRC sh
   tipsy config
   #+END_SRC

5. Generate the sample traffic traces that will be fed to the SUT during
   the benchmark (this may take a while).

   #+BEGIN_SRC sh
   tipsy traffic-gen
   #+END_SRC

6. Run the benchmarks (this may take an even longer while).

   #+BEGIN_SRC sh
   tipsy run
   #+END_SRC

7. Evaluate results: convert backend specific logs into a format that the
   visualization scripts understand.

   #+BEGIN_SRC sh
   tipsy evaluate
   #+END_SRC

8. Visualize the benchmark results.

   #+BEGIN_SRC sh
   tipsy visualize
   #+END_SRC

   In the above config, this will use the system default visualization
   tool (matplotlib of python) to create a single plot that will
   compare the packet rate as produced by the BESS and the OVS
   backends as the function of the scaling parameter (the number of
   users).

9. Optionally, you can issue a single command that will do all the above
   steps for you.

   #+BEGIN_SRC sh
   tipsy make
   #+END_SRC

   Note that all the above TIPSY commands will cleverly find out which
   steps they need to perform to be able to run, that is, =tipsy run= will
   call =tipsy config= to generate the configs (unless the configs are
   already available).

10. Finally, clean up the benchmark directory by removing all temporary
    files (pcaps, logs, etc.).

    #+BEGIN_SRC sh
    tipsy clean
    #+END_SRC
